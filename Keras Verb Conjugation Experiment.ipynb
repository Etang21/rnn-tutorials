{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation of sequence to sequence learning for performing addition\n",
    "Input: \"535+61\"\n",
    "Output: \"596\"\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "Input may optionally be reversed, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "\n",
    "Modified to train RNN to conjugate past tense of verbs. Data currently taken from UCLA dataset on verb conjugation.\n",
    "\n",
    "Code taken, modified from https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etang/anaconda3/envs/nlu/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import csv\n",
    "from keras.regularizers import L1L2\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Table\n",
    "\n",
    "Taking in our string of numbers/operations as a string of characters, we must vectorize each character as a new input to our RNN. \n",
    "\n",
    "We first create a table mapping each character in our vocabulary (in our case, this is the numbers 0-9 and addition symbol '+') to a unique integer index. We also create a table mapping back the other way. \n",
    "\n",
    "To encode a string, we convert each character in the string to a one-hot vector with the character index as 1 and all other indices as 0. We also feed in the parameter `num_rows` as a maximum length of our string, equivalent to padding all strings to a specified length.\n",
    "\n",
    "To decode an output array x, we take each vector of the string (representing a component) and decode it back into its most likely character. NOTE: I'm a little unclear about how we deal with variable-length answers. Maybe those are just decoded into space characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "These parameters are now heuristically chosen based on the data in our database.\n",
    "\n",
    "MAXLEN is the maximum length of any one input (or can be truncated below that). Chosen as 14, for max input length from UCLA dataset.  \n",
    "TRAINING_SIZE is an approximation (I think) for the number of examples in our dataset.  \n",
    "`chars` is a string with all characters that can be used in our inputs/outputs. We also add a space for padding.\n",
    "\n",
    "REVERSE is not currently used, but it should indicate whether we feed the inputs in standard order or reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "MAXLEN_INPUT = 14\n",
    "MAXLEN_OUTPUT = 15\n",
    "REVERSE = True\n",
    "\n",
    "chars = 'abcdefghijklmnopqrstuvwxyz '\n",
    "ctable = CharacterTable(chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing for Verb Tenses\n",
    "\n",
    "We read in the data from our CSV of UCLA's verb tenses. Then we vectorize them using the Character class and model parameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('       nodnaba', 'abandoned      '), ('         esaba', 'abased         '), ('         hsaba', 'abashed        '), ('         etaba', 'abated         '), ('    etaiverbba', 'abbreviated    '), ('      etacidba', 'abdicated      '), ('        tcudba', 'abducted       '), ('          teba', 'abetted        '), ('         rohba', 'abhorred       '), ('         ediba', 'abode          ')]\n",
      "MAXLEN of input:  16\n",
      "MAXLEN of output:  17\n",
      "Num examples:  6872\n"
     ]
    }
   ],
   "source": [
    "data_file = 'wordlist.csv'\n",
    "\n",
    "import re # For stripping non-alphanumeric characters\n",
    "present = []\n",
    "past = []\n",
    "with open(data_file) as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        if not (re.search(r'\\W+', row[0]) or re.search(r'\\W+', row[1])):\n",
    "            # Only add if all alphanumeric\n",
    "            # Then pad inputs with spaces\n",
    "            present_word = row[0] + ' ' * (MAXLEN_INPUT - len(row[0]))\n",
    "            if REVERSE:\n",
    "                # Reverse the query, e.g., '12+345  ' becomes '  543+21'.\n",
    "                present_word = present_word[::-1]\n",
    "            present.append(present_word)\n",
    "            past_word = row[1] + ' ' * (MAXLEN_OUTPUT - len(row[1]))\n",
    "            past.append(past_word)\n",
    "\n",
    "print(list(zip(present, past))[:10])\n",
    "print(\"MAXLEN of input: \", max([len(s) for s in present]))\n",
    "print(\"MAXLEN of output: \", max([len(s) for s in past]))\n",
    "\n",
    "assert(len(present)==len(past))\n",
    "print(\"Num examples: \", len(present))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Vectorize data using the Character library and previously set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Training Data:\n",
      "(1930, 14, 27)\n",
      "(1930, 15, 27)\n",
      "Validation Data:\n",
      "(214, 14, 27)\n",
      "(214, 15, 27)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(past), MAXLEN_INPUT, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(past), MAXLEN_OUTPUT, len(chars)), dtype=np.bool)\n",
    "for i, word in enumerate(present):\n",
    "    x[i] = ctable.encode(word, MAXLEN_INPUT)\n",
    "for i, word in enumerate(past):\n",
    "    y[i] = ctable.encode(word, MAXLEN_OUTPUT)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, 512)               1105920   \n",
      "_________________________________________________________________\n",
      "repeat_vector_11 (RepeatVect (None, 15, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 15, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 15, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 15, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 15, 27)            13851     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 15, 27)            0         \n",
      "=================================================================\n",
      "Total params: 7,417,371\n",
      "Trainable params: 7,417,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 512\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 3\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN_INPUT, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(MAXLEN_OUTPUT))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print validation examples and classifications from the dataset\n",
    "def classify_val_examples():\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)\n",
    "\n",
    "#Takes list of past loss and val_loss values, plots them\n",
    "def plot_learning_curves(loss, val_loss):\n",
    "    pyplot.clf()\n",
    "    pyplot.plot(loss)\n",
    "    pyplot.plot(val_loss)\n",
    "    pyplot.title('model train vs validation loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 1930 samples, validate on 214 samples\n",
      "Epoch 1/1\n",
      "1930/1930 [==============================] - 20s 10ms/step - loss: 1.4069 - acc: 0.5840 - val_loss: 1.4501 - val_acc: 0.5617\n",
      "Q please         T pleased         \u001b[91m☒\u001b[0m eeeeeed        \n",
      "Q ponder         T pondered        \u001b[91m☒\u001b[0m eeeeeed        \n",
      "Q coordinate     T coordinated     \u001b[91m☒\u001b[0m eeeeeeeeed     \n",
      "Q reward         T rewarded        \u001b[91m☒\u001b[0m eeeeeed        \n",
      "Q worry          T worried         \u001b[91m☒\u001b[0m eeeeee         \n",
      "Q complement     T complemented    \u001b[91m☒\u001b[0m eeeeeeeeed     \n",
      "Q foster         T fostered        \u001b[91m☒\u001b[0m eeeeeed        \n",
      "Q bay            T bayed           \u001b[91m☒\u001b[0m eeeee          \n",
      "Q core           T cored           \u001b[91m☒\u001b[0m eeeeed         \n",
      "Q weigh          T weighed         \u001b[91m☒\u001b[0m eeeeed         \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 1930 samples, validate on 214 samples\n",
      "Epoch 1/1\n",
      "1930/1930 [==============================] - 21s 11ms/step - loss: 1.3701 - acc: 0.5929 - val_loss: 1.6657 - val_acc: 0.4788\n",
      "Q glide          T glided          \u001b[91m☒\u001b[0m eeeeeed        \n",
      "Q contain        T contained       \u001b[91m☒\u001b[0m eeeeeeeeedd    \n",
      "Q expect         T expected        \u001b[91m☒\u001b[0m eeeeeeedd      \n",
      "Q paralyze       T paralyzed       \u001b[91m☒\u001b[0m eeeeeeeeeedd   \n",
      "Q rule           T ruled           \u001b[91m☒\u001b[0m eeeeeed        \n",
      "Q converge       T converged       \u001b[91m☒\u001b[0m eeeeeeeeeed    \n",
      "Q admonish       T admonished      \u001b[91m☒\u001b[0m aeeeeeeeeeed   \n",
      "Q approximate    T approximated    \u001b[91m☒\u001b[0m aeeeeeeeeeeedd \n",
      "Q weigh          T weighed         \u001b[91m☒\u001b[0m eeeeeeed       \n",
      "Q jab            T jabbed          \u001b[91m☒\u001b[0m eeeeed         \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 1930 samples, validate on 214 samples\n",
      "Epoch 1/1\n",
      " 384/1930 [====>.........................] - ETA: 15s - loss: 1.5110 - acc: 0.5304"
     ]
    }
   ],
   "source": [
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset. Also graph the loss and validation loss.\n",
    "\n",
    "loss = []\n",
    "val_loss = []\n",
    "for iteration in range(1, 500):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    loss.append(history.history['loss'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    if iteration % 1 == 0:\n",
    "        classify_val_examples()\n",
    "    if iteration % 50 == 0:\n",
    "        plot_learning_curves(loss, val_loss)\n",
    "    \n",
    "pyplot.title('model train vs validation loss')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Underfitting, Overfitting\n",
    "\n",
    "Here we plot the model's results to diagnose underfitting or overfitting.\n",
    "\n",
    "We'll plot the learning curves with training loss and validation loss to understand what happens. Machine Learning Yearning might help us understand this too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1930 samples, validate on 214 samples\n",
      "Epoch 1/600\n",
      "1930/1930 [==============================] - 3s 2ms/step - loss: 16.2367 - acc: 0.4288 - val_loss: 13.1607 - val_acc: 0.5012\n",
      "Epoch 2/600\n",
      "1930/1930 [==============================] - 1s 615us/step - loss: 11.3721 - acc: 0.4943 - val_loss: 9.2820 - val_acc: 0.5012\n",
      "Epoch 3/600\n",
      "1930/1930 [==============================] - 1s 660us/step - loss: 7.9315 - acc: 0.5038 - val_loss: 6.3251 - val_acc: 0.5274\n",
      "Epoch 4/600\n",
      "1930/1930 [==============================] - 1s 617us/step - loss: 5.3043 - acc: 0.5392 - val_loss: 4.1224 - val_acc: 0.5757\n",
      "Epoch 5/600\n",
      "1930/1930 [==============================] - 1s 667us/step - loss: 3.4177 - acc: 0.5678 - val_loss: 2.6190 - val_acc: 0.5732\n",
      "Epoch 6/600\n",
      "1930/1930 [==============================] - 1s 621us/step - loss: 2.2065 - acc: 0.5794 - val_loss: 1.7840 - val_acc: 0.5819\n",
      "Epoch 7/600\n",
      "1930/1930 [==============================] - 1s 609us/step - loss: 1.6438 - acc: 0.5798 - val_loss: 1.5287 - val_acc: 0.5869\n",
      "Epoch 8/600\n",
      "1930/1930 [==============================] - 1s 605us/step - loss: 1.4950 - acc: 0.5897 - val_loss: 1.4427 - val_acc: 0.5978\n",
      "Epoch 9/600\n",
      "1930/1930 [==============================] - 1s 614us/step - loss: 1.4288 - acc: 0.5928 - val_loss: 1.4747 - val_acc: 0.5801\n",
      "Epoch 10/600\n",
      "1930/1930 [==============================] - 1s 616us/step - loss: 1.4566 - acc: 0.5697 - val_loss: 1.3939 - val_acc: 0.5816\n",
      "Epoch 11/600\n",
      "1930/1930 [==============================] - 1s 617us/step - loss: 1.3971 - acc: 0.5893 - val_loss: 1.3685 - val_acc: 0.5925\n",
      "Epoch 12/600\n",
      "1930/1930 [==============================] - 1s 633us/step - loss: 1.3771 - acc: 0.5921 - val_loss: 1.3805 - val_acc: 0.6016\n",
      "Epoch 13/600\n",
      "1930/1930 [==============================] - 1s 629us/step - loss: 1.3746 - acc: 0.5905 - val_loss: 1.5102 - val_acc: 0.5601\n",
      "Epoch 14/600\n",
      "1930/1930 [==============================] - 1s 632us/step - loss: 1.4502 - acc: 0.5650 - val_loss: 1.3829 - val_acc: 0.5807\n",
      "Epoch 15/600\n",
      "1930/1930 [==============================] - 1s 627us/step - loss: 1.3876 - acc: 0.5856 - val_loss: 1.3528 - val_acc: 0.6025\n",
      "Epoch 16/600\n",
      "1930/1930 [==============================] - 1s 634us/step - loss: 1.3671 - acc: 0.5907 - val_loss: 1.3432 - val_acc: 0.6040\n",
      "Epoch 17/600\n",
      "1930/1930 [==============================] - 1s 635us/step - loss: 1.3541 - acc: 0.5954 - val_loss: 1.3370 - val_acc: 0.6025\n",
      "Epoch 18/600\n",
      "1930/1930 [==============================] - 1s 632us/step - loss: 1.3485 - acc: 0.5949 - val_loss: 1.3391 - val_acc: 0.5919\n",
      "Epoch 19/600\n",
      "1930/1930 [==============================] - 1s 628us/step - loss: 1.3428 - acc: 0.5935 - val_loss: 1.3620 - val_acc: 0.5838\n",
      "Epoch 20/600\n",
      "1930/1930 [==============================] - 1s 633us/step - loss: 1.3534 - acc: 0.5866 - val_loss: 1.3664 - val_acc: 0.5854\n",
      "Epoch 21/600\n",
      "1930/1930 [==============================] - 1s 636us/step - loss: 1.3428 - acc: 0.5930 - val_loss: 1.3339 - val_acc: 0.5897\n",
      "Epoch 22/600\n",
      "1930/1930 [==============================] - 1s 632us/step - loss: 1.3381 - acc: 0.5933 - val_loss: 1.4077 - val_acc: 0.5804\n",
      "Epoch 23/600\n",
      "1930/1930 [==============================] - 1s 680us/step - loss: 1.3452 - acc: 0.5910 - val_loss: 1.3705 - val_acc: 0.5779\n",
      "Epoch 24/600\n",
      "1930/1930 [==============================] - 2s 788us/step - loss: 1.3357 - acc: 0.5933 - val_loss: 1.3146 - val_acc: 0.6022\n",
      "Epoch 25/600\n",
      "1930/1930 [==============================] - 2s 785us/step - loss: 1.3293 - acc: 0.5947 - val_loss: 1.3160 - val_acc: 0.5981\n",
      "Epoch 26/600\n",
      "1930/1930 [==============================] - 1s 759us/step - loss: 1.3300 - acc: 0.5967 - val_loss: 1.3148 - val_acc: 0.6009\n",
      "Epoch 27/600\n",
      "1930/1930 [==============================] - 1s 768us/step - loss: 1.3260 - acc: 0.5954 - val_loss: 1.4104 - val_acc: 0.5748\n",
      "Epoch 28/600\n",
      "1930/1930 [==============================] - 2s 803us/step - loss: 1.3418 - acc: 0.5896 - val_loss: 1.3349 - val_acc: 0.6000\n",
      "Epoch 29/600\n",
      "1930/1930 [==============================] - 1s 777us/step - loss: 1.3351 - acc: 0.5926 - val_loss: 1.3101 - val_acc: 0.6047\n",
      "Epoch 30/600\n",
      "1930/1930 [==============================] - 1s 759us/step - loss: 1.3282 - acc: 0.5968 - val_loss: 1.3707 - val_acc: 0.5816\n",
      "Epoch 31/600\n",
      "1930/1930 [==============================] - 2s 786us/step - loss: 1.3404 - acc: 0.5926 - val_loss: 1.3036 - val_acc: 0.6056\n",
      "Epoch 32/600\n",
      "1930/1930 [==============================] - 1s 742us/step - loss: 1.3169 - acc: 0.5990 - val_loss: 1.3056 - val_acc: 0.6062\n",
      "Epoch 33/600\n",
      "1930/1930 [==============================] - 1s 776us/step - loss: 1.3132 - acc: 0.5985 - val_loss: 1.2986 - val_acc: 0.6056\n",
      "Epoch 34/600\n",
      "1930/1930 [==============================] - 2s 791us/step - loss: 1.3148 - acc: 0.5997 - val_loss: 1.3007 - val_acc: 0.6084\n",
      "Epoch 35/600\n",
      "1930/1930 [==============================] - 1s 769us/step - loss: 1.3110 - acc: 0.6016 - val_loss: 1.3025 - val_acc: 0.6065\n",
      "Epoch 36/600\n",
      "1930/1930 [==============================] - 2s 809us/step - loss: 1.3043 - acc: 0.6043 - val_loss: 1.3111 - val_acc: 0.6006\n",
      "Epoch 37/600\n",
      "1930/1930 [==============================] - 2s 799us/step - loss: 1.3021 - acc: 0.6059 - val_loss: 1.2919 - val_acc: 0.6103\n",
      "Epoch 38/600\n",
      "1930/1930 [==============================] - 2s 881us/step - loss: 1.2962 - acc: 0.6102 - val_loss: 1.2836 - val_acc: 0.6199\n",
      "Epoch 39/600\n",
      "1930/1930 [==============================] - 2s 809us/step - loss: 1.2955 - acc: 0.6146 - val_loss: 1.3217 - val_acc: 0.5944\n",
      "Epoch 40/600\n",
      "1930/1930 [==============================] - 2s 784us/step - loss: 1.3007 - acc: 0.6093 - val_loss: 1.3115 - val_acc: 0.6031\n",
      "Epoch 41/600\n",
      "1930/1930 [==============================] - 1s 740us/step - loss: 1.2996 - acc: 0.6137 - val_loss: 1.2840 - val_acc: 0.6156\n",
      "Epoch 42/600\n",
      "1930/1930 [==============================] - 1s 704us/step - loss: 1.2925 - acc: 0.6166 - val_loss: 1.2957 - val_acc: 0.6165\n",
      "Epoch 43/600\n",
      "1930/1930 [==============================] - 1s 717us/step - loss: 1.2886 - acc: 0.6191 - val_loss: 1.2645 - val_acc: 0.6293\n",
      "Epoch 44/600\n",
      "1930/1930 [==============================] - 1s 717us/step - loss: 1.2774 - acc: 0.6247 - val_loss: 1.2651 - val_acc: 0.6290\n",
      "Epoch 45/600\n",
      "1930/1930 [==============================] - 2s 813us/step - loss: 1.2711 - acc: 0.6322 - val_loss: 1.2567 - val_acc: 0.6355\n",
      "Epoch 46/600\n",
      "1930/1930 [==============================] - 2s 874us/step - loss: 1.2764 - acc: 0.6244 - val_loss: 1.2604 - val_acc: 0.6299\n",
      "Epoch 47/600\n",
      "1930/1930 [==============================] - 1s 749us/step - loss: 1.2793 - acc: 0.6230 - val_loss: 1.2597 - val_acc: 0.6312\n",
      "Epoch 48/600\n",
      "1930/1930 [==============================] - 1s 738us/step - loss: 1.2841 - acc: 0.6182 - val_loss: 1.2556 - val_acc: 0.6308\n",
      "Epoch 49/600\n",
      "1930/1930 [==============================] - 1s 647us/step - loss: 1.2813 - acc: 0.6240 - val_loss: 1.2892 - val_acc: 0.6150\n",
      "Epoch 50/600\n",
      "1930/1930 [==============================] - 1s 586us/step - loss: 1.3147 - acc: 0.6123 - val_loss: 1.2658 - val_acc: 0.6249\n",
      "Epoch 51/600\n",
      "1930/1930 [==============================] - 1s 589us/step - loss: 1.2804 - acc: 0.6240 - val_loss: 1.2518 - val_acc: 0.6399\n",
      "Epoch 52/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.2622 - acc: 0.6355 - val_loss: 1.2749 - val_acc: 0.6268\n",
      "Epoch 53/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.2678 - acc: 0.6324 - val_loss: 1.2436 - val_acc: 0.6445\n",
      "Epoch 54/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.2599 - acc: 0.6369 - val_loss: 1.2432 - val_acc: 0.6436\n",
      "Epoch 55/600\n",
      "1930/1930 [==============================] - 1s 583us/step - loss: 1.2520 - acc: 0.6406 - val_loss: 1.2398 - val_acc: 0.6405\n",
      "Epoch 56/600\n",
      "1930/1930 [==============================] - 1s 591us/step - loss: 1.2492 - acc: 0.6423 - val_loss: 1.2345 - val_acc: 0.6439\n",
      "Epoch 57/600\n",
      "1930/1930 [==============================] - 1s 582us/step - loss: 1.2505 - acc: 0.6401 - val_loss: 1.2624 - val_acc: 0.6308\n",
      "Epoch 58/600\n",
      "1930/1930 [==============================] - 1s 577us/step - loss: 1.2615 - acc: 0.6353 - val_loss: 1.2347 - val_acc: 0.6427\n",
      "Epoch 59/600\n",
      "1930/1930 [==============================] - 1s 576us/step - loss: 1.2549 - acc: 0.6406 - val_loss: 1.2359 - val_acc: 0.6455\n",
      "Epoch 60/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930/1930 [==============================] - 1s 589us/step - loss: 1.2479 - acc: 0.6424 - val_loss: 1.3026 - val_acc: 0.6059\n",
      "Epoch 61/600\n",
      "1930/1930 [==============================] - 1s 658us/step - loss: 1.2657 - acc: 0.6327 - val_loss: 1.2475 - val_acc: 0.6293\n",
      "Epoch 62/600\n",
      "1930/1930 [==============================] - 1s 682us/step - loss: 1.2452 - acc: 0.6430 - val_loss: 1.2669 - val_acc: 0.6274\n",
      "Epoch 63/600\n",
      "1930/1930 [==============================] - 1s 713us/step - loss: 1.2403 - acc: 0.6461 - val_loss: 1.2296 - val_acc: 0.6436\n",
      "Epoch 64/600\n",
      "1930/1930 [==============================] - 1s 713us/step - loss: 1.2300 - acc: 0.6527 - val_loss: 1.2156 - val_acc: 0.6592\n",
      "Epoch 65/600\n",
      "1930/1930 [==============================] - 1s 703us/step - loss: 1.2233 - acc: 0.6529 - val_loss: 1.2466 - val_acc: 0.6308\n",
      "Epoch 66/600\n",
      "1930/1930 [==============================] - 1s 687us/step - loss: 1.2351 - acc: 0.6473 - val_loss: 1.2176 - val_acc: 0.6498\n",
      "Epoch 67/600\n",
      "1930/1930 [==============================] - 1s 600us/step - loss: 1.2239 - acc: 0.6546 - val_loss: 1.2237 - val_acc: 0.6567\n",
      "Epoch 68/600\n",
      "1930/1930 [==============================] - 1s 580us/step - loss: 1.2205 - acc: 0.6547 - val_loss: 1.2029 - val_acc: 0.6604\n",
      "Epoch 69/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.2154 - acc: 0.6561 - val_loss: 1.2168 - val_acc: 0.6579\n",
      "Epoch 70/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.2207 - acc: 0.6532 - val_loss: 1.2470 - val_acc: 0.6349\n",
      "Epoch 71/600\n",
      "1930/1930 [==============================] - 1s 582us/step - loss: 1.2546 - acc: 0.6414 - val_loss: 1.2076 - val_acc: 0.6567\n",
      "Epoch 72/600\n",
      "1930/1930 [==============================] - 1s 588us/step - loss: 1.2187 - acc: 0.6562 - val_loss: 1.2070 - val_acc: 0.6586\n",
      "Epoch 73/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.2100 - acc: 0.6600 - val_loss: 1.1932 - val_acc: 0.6654\n",
      "Epoch 74/600\n",
      "1930/1930 [==============================] - 1s 588us/step - loss: 1.2010 - acc: 0.6619 - val_loss: 1.2002 - val_acc: 0.6586\n",
      "Epoch 75/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.2020 - acc: 0.6617 - val_loss: 1.1902 - val_acc: 0.6642\n",
      "Epoch 76/600\n",
      "1930/1930 [==============================] - 1s 588us/step - loss: 1.1965 - acc: 0.6620 - val_loss: 1.1953 - val_acc: 0.6604\n",
      "Epoch 77/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 1.2094 - acc: 0.6572 - val_loss: 1.2604 - val_acc: 0.6324\n",
      "Epoch 78/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.2292 - acc: 0.6483 - val_loss: 1.1977 - val_acc: 0.6614\n",
      "Epoch 79/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 1.1973 - acc: 0.6622 - val_loss: 1.2241 - val_acc: 0.6442\n",
      "Epoch 80/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 1.2045 - acc: 0.6593 - val_loss: 1.1864 - val_acc: 0.6645\n",
      "Epoch 81/600\n",
      "1930/1930 [==============================] - 1s 589us/step - loss: 1.1903 - acc: 0.6643 - val_loss: 1.1811 - val_acc: 0.6642\n",
      "Epoch 82/600\n",
      "1930/1930 [==============================] - 1s 576us/step - loss: 1.1870 - acc: 0.6640 - val_loss: 1.1871 - val_acc: 0.6592\n",
      "Epoch 83/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.1840 - acc: 0.6643 - val_loss: 1.1755 - val_acc: 0.6664\n",
      "Epoch 84/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.1885 - acc: 0.6633 - val_loss: 1.1826 - val_acc: 0.6607\n",
      "Epoch 85/600\n",
      "1930/1930 [==============================] - 1s 577us/step - loss: 1.2004 - acc: 0.6580 - val_loss: 1.1788 - val_acc: 0.6682\n",
      "Epoch 86/600\n",
      "1930/1930 [==============================] - 1s 590us/step - loss: 1.1860 - acc: 0.6647 - val_loss: 1.1703 - val_acc: 0.6673\n",
      "Epoch 87/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.1809 - acc: 0.6654 - val_loss: 1.1967 - val_acc: 0.6564\n",
      "Epoch 88/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.1780 - acc: 0.6651 - val_loss: 1.1677 - val_acc: 0.6679\n",
      "Epoch 89/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.1847 - acc: 0.6625 - val_loss: 1.1973 - val_acc: 0.6539\n",
      "Epoch 90/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.1848 - acc: 0.6645 - val_loss: 1.1897 - val_acc: 0.6695\n",
      "Epoch 91/600\n",
      "1930/1930 [==============================] - 1s 583us/step - loss: 1.1756 - acc: 0.6682 - val_loss: 1.1619 - val_acc: 0.6729\n",
      "Epoch 92/600\n",
      "1930/1930 [==============================] - 1s 584us/step - loss: 1.1659 - acc: 0.6703 - val_loss: 1.1851 - val_acc: 0.6595\n",
      "Epoch 93/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.1682 - acc: 0.6679 - val_loss: 1.1615 - val_acc: 0.6707\n",
      "Epoch 94/600\n",
      "1930/1930 [==============================] - 1s 568us/step - loss: 1.1621 - acc: 0.6712 - val_loss: 1.2198 - val_acc: 0.6505\n",
      "Epoch 95/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.1747 - acc: 0.6656 - val_loss: 1.1694 - val_acc: 0.6698\n",
      "Epoch 96/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.1938 - acc: 0.6593 - val_loss: 1.1670 - val_acc: 0.6710\n",
      "Epoch 97/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 1.1717 - acc: 0.6692 - val_loss: 1.1519 - val_acc: 0.6729\n",
      "Epoch 98/600\n",
      "1930/1930 [==============================] - 1s 586us/step - loss: 1.1549 - acc: 0.6723 - val_loss: 1.1428 - val_acc: 0.6760\n",
      "Epoch 99/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.1470 - acc: 0.6734 - val_loss: 1.1373 - val_acc: 0.6785\n",
      "Epoch 100/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.1422 - acc: 0.6747 - val_loss: 1.1759 - val_acc: 0.6611\n",
      "Epoch 101/600\n",
      "1930/1930 [==============================] - 1s 619us/step - loss: 1.2130 - acc: 0.6507 - val_loss: 1.1647 - val_acc: 0.6654\n",
      "Epoch 102/600\n",
      "1930/1930 [==============================] - 1s 610us/step - loss: 1.1787 - acc: 0.6661 - val_loss: 1.1478 - val_acc: 0.6766\n",
      "Epoch 103/600\n",
      "1930/1930 [==============================] - 1s 592us/step - loss: 1.1532 - acc: 0.6723 - val_loss: 1.1430 - val_acc: 0.6788\n",
      "Epoch 104/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.1574 - acc: 0.6683 - val_loss: 1.1732 - val_acc: 0.6586\n",
      "Epoch 105/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.1521 - acc: 0.6713 - val_loss: 1.1292 - val_acc: 0.6757\n",
      "Epoch 106/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.1351 - acc: 0.6763 - val_loss: 1.1234 - val_acc: 0.6794\n",
      "Epoch 107/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.1319 - acc: 0.6769 - val_loss: 1.1249 - val_acc: 0.6773\n",
      "Epoch 108/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.1374 - acc: 0.6761 - val_loss: 1.1491 - val_acc: 0.6738\n",
      "Epoch 109/600\n",
      "1930/1930 [==============================] - 1s 583us/step - loss: 1.1369 - acc: 0.6764 - val_loss: 1.1239 - val_acc: 0.6773\n",
      "Epoch 110/600\n",
      "1930/1930 [==============================] - 1s 588us/step - loss: 1.1220 - acc: 0.6798 - val_loss: 1.1293 - val_acc: 0.6754\n",
      "Epoch 111/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.1197 - acc: 0.6799 - val_loss: 1.1036 - val_acc: 0.6832\n",
      "Epoch 112/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.1140 - acc: 0.6815 - val_loss: 1.1243 - val_acc: 0.6717\n",
      "Epoch 113/600\n",
      "1930/1930 [==============================] - 1s 569us/step - loss: 1.1203 - acc: 0.6796 - val_loss: 1.1093 - val_acc: 0.6847\n",
      "Epoch 114/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.1557 - acc: 0.6708 - val_loss: 1.2563 - val_acc: 0.6293\n",
      "Epoch 115/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.1944 - acc: 0.6574 - val_loss: 1.1278 - val_acc: 0.6826\n",
      "Epoch 116/600\n",
      "1930/1930 [==============================] - 1s 581us/step - loss: 1.1351 - acc: 0.6770 - val_loss: 1.1177 - val_acc: 0.6822\n",
      "Epoch 117/600\n",
      "1930/1930 [==============================] - 1s 587us/step - loss: 1.1189 - acc: 0.6815 - val_loss: 1.1002 - val_acc: 0.6891\n",
      "Epoch 118/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.1137 - acc: 0.6841 - val_loss: 1.0991 - val_acc: 0.6866\n",
      "Epoch 119/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.1067 - acc: 0.6840 - val_loss: 1.0962 - val_acc: 0.6879\n",
      "Epoch 120/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.1007 - acc: 0.6862 - val_loss: 1.1137 - val_acc: 0.6854\n",
      "Epoch 121/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.1091 - acc: 0.6839 - val_loss: 1.0905 - val_acc: 0.6850\n",
      "Epoch 122/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.0993 - acc: 0.6856 - val_loss: 1.0889 - val_acc: 0.6888\n",
      "Epoch 123/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.0974 - acc: 0.6877 - val_loss: 1.0866 - val_acc: 0.6882\n",
      "Epoch 124/600\n",
      "1930/1930 [==============================] - 1s 589us/step - loss: 1.0952 - acc: 0.6888 - val_loss: 1.0833 - val_acc: 0.6894\n",
      "Epoch 125/600\n",
      "1930/1930 [==============================] - 1s 581us/step - loss: 1.0941 - acc: 0.6881 - val_loss: 1.0956 - val_acc: 0.6841\n",
      "Epoch 126/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.0950 - acc: 0.6889 - val_loss: 1.0946 - val_acc: 0.6897\n",
      "Epoch 127/600\n",
      "1930/1930 [==============================] - 1s 584us/step - loss: 1.0993 - acc: 0.6874 - val_loss: 1.0920 - val_acc: 0.6891\n",
      "Epoch 128/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.0977 - acc: 0.6887 - val_loss: 1.0920 - val_acc: 0.6891\n",
      "Epoch 129/600\n",
      "1930/1930 [==============================] - 1s 588us/step - loss: 1.0895 - acc: 0.6907 - val_loss: 1.0993 - val_acc: 0.6841\n",
      "Epoch 130/600\n",
      "1930/1930 [==============================] - 1s 579us/step - loss: 1.0969 - acc: 0.6880 - val_loss: 1.2052 - val_acc: 0.6383\n",
      "Epoch 131/600\n",
      "1930/1930 [==============================] - 1s 584us/step - loss: 1.1840 - acc: 0.6609 - val_loss: 1.1630 - val_acc: 0.6717\n",
      "Epoch 132/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.1463 - acc: 0.6746 - val_loss: 1.1008 - val_acc: 0.6897\n",
      "Epoch 133/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.0953 - acc: 0.6906 - val_loss: 1.0785 - val_acc: 0.6941\n",
      "Epoch 134/600\n",
      "1930/1930 [==============================] - 1s 583us/step - loss: 1.0813 - acc: 0.6926 - val_loss: 1.0990 - val_acc: 0.6854\n",
      "Epoch 135/600\n",
      "1930/1930 [==============================] - 1s 576us/step - loss: 1.1016 - acc: 0.6871 - val_loss: 1.0815 - val_acc: 0.6953\n",
      "Epoch 136/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.0837 - acc: 0.6931 - val_loss: 1.0742 - val_acc: 0.6966\n",
      "Epoch 137/600\n",
      "1930/1930 [==============================] - 1s 582us/step - loss: 1.0779 - acc: 0.6933 - val_loss: 1.0717 - val_acc: 0.6919\n",
      "Epoch 138/600\n",
      "1930/1930 [==============================] - 1s 588us/step - loss: 1.0744 - acc: 0.6937 - val_loss: 1.0649 - val_acc: 0.6925\n",
      "Epoch 139/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.0697 - acc: 0.6941 - val_loss: 1.0622 - val_acc: 0.6928\n",
      "Epoch 140/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.0668 - acc: 0.6950 - val_loss: 1.0635 - val_acc: 0.6963\n",
      "Epoch 141/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.0665 - acc: 0.6948 - val_loss: 1.0760 - val_acc: 0.6925\n",
      "Epoch 142/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.0707 - acc: 0.6931 - val_loss: 1.0662 - val_acc: 0.6966\n",
      "Epoch 143/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.0687 - acc: 0.6960 - val_loss: 1.0613 - val_acc: 0.6922\n",
      "Epoch 144/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 1.0676 - acc: 0.6945 - val_loss: 1.1225 - val_acc: 0.6760\n",
      "Epoch 145/600\n",
      "1930/1930 [==============================] - 1s 589us/step - loss: 1.1014 - acc: 0.6830 - val_loss: 1.0684 - val_acc: 0.6969\n",
      "Epoch 146/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.0771 - acc: 0.6952 - val_loss: 1.0715 - val_acc: 0.6944\n",
      "Epoch 147/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.0832 - acc: 0.6909 - val_loss: 1.0718 - val_acc: 0.6991\n",
      "Epoch 148/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.0667 - acc: 0.6936 - val_loss: 1.0557 - val_acc: 0.6960\n",
      "Epoch 149/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.0542 - acc: 0.6961 - val_loss: 1.0594 - val_acc: 0.6994\n",
      "Epoch 150/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.0592 - acc: 0.6957 - val_loss: 1.0554 - val_acc: 0.6978\n",
      "Epoch 151/600\n",
      "1930/1930 [==============================] - 1s 581us/step - loss: 1.0605 - acc: 0.6962 - val_loss: 1.0709 - val_acc: 0.6972\n",
      "Epoch 152/600\n",
      "1930/1930 [==============================] - 1s 584us/step - loss: 1.0988 - acc: 0.6888 - val_loss: 1.0791 - val_acc: 0.6922\n",
      "Epoch 153/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.0713 - acc: 0.6934 - val_loss: 1.0630 - val_acc: 0.6981\n",
      "Epoch 154/600\n",
      "1930/1930 [==============================] - 1s 585us/step - loss: 1.0555 - acc: 0.6954 - val_loss: 1.0503 - val_acc: 0.6963\n",
      "Epoch 155/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.0487 - acc: 0.6971 - val_loss: 1.0475 - val_acc: 0.7000\n",
      "Epoch 156/600\n",
      "1930/1930 [==============================] - 1s 581us/step - loss: 1.0464 - acc: 0.6974 - val_loss: 1.0807 - val_acc: 0.6844\n",
      "Epoch 157/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.0644 - acc: 0.6931 - val_loss: 1.0569 - val_acc: 0.6913\n",
      "Epoch 158/600\n",
      "1930/1930 [==============================] - 1s 588us/step - loss: 1.0499 - acc: 0.6984 - val_loss: 1.0487 - val_acc: 0.7006\n",
      "Epoch 159/600\n",
      "1930/1930 [==============================] - 1s 580us/step - loss: 1.0404 - acc: 0.7003 - val_loss: 1.0559 - val_acc: 0.6953\n",
      "Epoch 160/600\n",
      "1930/1930 [==============================] - 1s 569us/step - loss: 1.0417 - acc: 0.6979 - val_loss: 1.0609 - val_acc: 0.6938\n",
      "Epoch 161/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.0532 - acc: 0.6957 - val_loss: 1.0447 - val_acc: 0.6972\n",
      "Epoch 162/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.0377 - acc: 0.6994 - val_loss: 1.0569 - val_acc: 0.6963\n",
      "Epoch 163/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.0436 - acc: 0.6996 - val_loss: 1.0530 - val_acc: 0.6978\n",
      "Epoch 164/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 1.0336 - acc: 0.7012 - val_loss: 1.0371 - val_acc: 0.6981\n",
      "Epoch 165/600\n",
      "1930/1930 [==============================] - 1s 580us/step - loss: 1.0406 - acc: 0.6989 - val_loss: 1.1128 - val_acc: 0.6829\n",
      "Epoch 166/600\n",
      "1930/1930 [==============================] - 1s 598us/step - loss: 1.0616 - acc: 0.6964 - val_loss: 1.0461 - val_acc: 0.6978\n",
      "Epoch 167/600\n",
      "1930/1930 [==============================] - 1s 634us/step - loss: 1.0341 - acc: 0.7030 - val_loss: 1.0449 - val_acc: 0.7040\n",
      "Epoch 168/600\n",
      "1930/1930 [==============================] - 1s 613us/step - loss: 1.0318 - acc: 0.7017 - val_loss: 1.0290 - val_acc: 0.7031\n",
      "Epoch 169/600\n",
      "1930/1930 [==============================] - 1s 600us/step - loss: 1.0301 - acc: 0.7032 - val_loss: 1.0501 - val_acc: 0.6941\n",
      "Epoch 170/600\n",
      "1930/1930 [==============================] - 1s 586us/step - loss: 1.0338 - acc: 0.7012 - val_loss: 1.0411 - val_acc: 0.7037\n",
      "Epoch 171/600\n",
      "1930/1930 [==============================] - 1s 582us/step - loss: 1.0246 - acc: 0.7031 - val_loss: 1.0324 - val_acc: 0.7050\n",
      "Epoch 172/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.0208 - acc: 0.7040 - val_loss: 1.0299 - val_acc: 0.7003\n",
      "Epoch 173/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 1.0192 - acc: 0.7063 - val_loss: 1.0368 - val_acc: 0.7047\n",
      "Epoch 174/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.0385 - acc: 0.7000 - val_loss: 1.0319 - val_acc: 0.7000\n",
      "Epoch 175/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 1.0163 - acc: 0.7054 - val_loss: 1.0183 - val_acc: 0.7037\n",
      "Epoch 176/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 1.0102 - acc: 0.7062 - val_loss: 1.0225 - val_acc: 0.7037\n",
      "Epoch 177/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.0104 - acc: 0.7073 - val_loss: 1.0190 - val_acc: 0.7065\n",
      "Epoch 178/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.0149 - acc: 0.7060 - val_loss: 1.0364 - val_acc: 0.7016\n",
      "Epoch 179/600\n",
      "1930/1930 [==============================] - 1s 586us/step - loss: 1.0168 - acc: 0.7062 - val_loss: 1.0164 - val_acc: 0.7047\n",
      "Epoch 180/600\n",
      "1930/1930 [==============================] - 1s 587us/step - loss: 1.0034 - acc: 0.7087 - val_loss: 1.0323 - val_acc: 0.7016\n",
      "Epoch 181/600\n",
      "1930/1930 [==============================] - 1s 586us/step - loss: 1.0318 - acc: 0.7014 - val_loss: 1.1364 - val_acc: 0.6798\n",
      "Epoch 182/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.1142 - acc: 0.6820 - val_loss: 1.0871 - val_acc: 0.6916\n",
      "Epoch 183/600\n",
      "1930/1930 [==============================] - 1s 570us/step - loss: 1.0476 - acc: 0.6989 - val_loss: 1.0341 - val_acc: 0.7022\n",
      "Epoch 184/600\n",
      "1930/1930 [==============================] - 1s 570us/step - loss: 1.0126 - acc: 0.7062 - val_loss: 1.0250 - val_acc: 0.7019\n",
      "Epoch 185/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.0024 - acc: 0.7101 - val_loss: 1.0228 - val_acc: 0.7044\n",
      "Epoch 186/600\n",
      "1930/1930 [==============================] - 1s 580us/step - loss: 1.0134 - acc: 0.7081 - val_loss: 1.0227 - val_acc: 0.7087\n",
      "Epoch 187/600\n",
      "1930/1930 [==============================] - 1s 600us/step - loss: 1.0027 - acc: 0.7090 - val_loss: 1.0070 - val_acc: 0.7065\n",
      "Epoch 188/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 1.0083 - acc: 0.7076 - val_loss: 1.0267 - val_acc: 0.7022\n",
      "Epoch 189/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 1.0007 - acc: 0.7111 - val_loss: 1.0128 - val_acc: 0.7059\n",
      "Epoch 190/600\n",
      "1930/1930 [==============================] - 1s 590us/step - loss: 0.9976 - acc: 0.7104 - val_loss: 1.0252 - val_acc: 0.7072\n",
      "Epoch 191/600\n",
      "1930/1930 [==============================] - 1s 580us/step - loss: 0.9985 - acc: 0.7123 - val_loss: 1.0533 - val_acc: 0.6960\n",
      "Epoch 192/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 1.0619 - acc: 0.6919 - val_loss: 1.0846 - val_acc: 0.6847\n",
      "Epoch 193/600\n",
      "1930/1930 [==============================] - 1s 588us/step - loss: 1.0309 - acc: 0.7032 - val_loss: 1.0338 - val_acc: 0.7078\n",
      "Epoch 194/600\n",
      "1930/1930 [==============================] - 1s 582us/step - loss: 1.0166 - acc: 0.7073 - val_loss: 1.0203 - val_acc: 0.7087\n",
      "Epoch 195/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 0.9953 - acc: 0.7115 - val_loss: 1.0101 - val_acc: 0.7097\n",
      "Epoch 196/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.9994 - acc: 0.7096 - val_loss: 1.0034 - val_acc: 0.7097\n",
      "Epoch 197/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.9891 - acc: 0.7127 - val_loss: 1.0021 - val_acc: 0.7118\n",
      "Epoch 198/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.9833 - acc: 0.7144 - val_loss: 1.0038 - val_acc: 0.7075\n",
      "Epoch 199/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9823 - acc: 0.7150 - val_loss: 0.9994 - val_acc: 0.7131\n",
      "Epoch 200/600\n",
      "1930/1930 [==============================] - 1s 594us/step - loss: 0.9758 - acc: 0.7175 - val_loss: 1.0215 - val_acc: 0.7025\n",
      "Epoch 201/600\n",
      "1930/1930 [==============================] - 1s 580us/step - loss: 1.0065 - acc: 0.7066 - val_loss: 1.0204 - val_acc: 0.7084\n",
      "Epoch 202/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9869 - acc: 0.7140 - val_loss: 0.9998 - val_acc: 0.7134\n",
      "Epoch 203/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 0.9779 - acc: 0.7158 - val_loss: 1.0042 - val_acc: 0.7084\n",
      "Epoch 204/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.9729 - acc: 0.7178 - val_loss: 1.0178 - val_acc: 0.7069\n",
      "Epoch 205/600\n",
      "1930/1930 [==============================] - 1s 576us/step - loss: 0.9834 - acc: 0.7154 - val_loss: 1.0156 - val_acc: 0.7097\n",
      "Epoch 206/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9767 - acc: 0.7177 - val_loss: 1.0069 - val_acc: 0.7053\n",
      "Epoch 207/600\n",
      "1930/1930 [==============================] - 1s 583us/step - loss: 0.9757 - acc: 0.7162 - val_loss: 1.0325 - val_acc: 0.7034\n",
      "Epoch 208/600\n",
      "1930/1930 [==============================] - 1s 625us/step - loss: 1.0385 - acc: 0.7044 - val_loss: 1.1522 - val_acc: 0.6773\n",
      "Epoch 209/600\n",
      "1930/1930 [==============================] - 1s 595us/step - loss: 1.0602 - acc: 0.6984 - val_loss: 1.0336 - val_acc: 0.7034\n",
      "Epoch 210/600\n",
      "1930/1930 [==============================] - 1s 584us/step - loss: 1.0131 - acc: 0.7095 - val_loss: 1.0100 - val_acc: 0.7112\n",
      "Epoch 211/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.9921 - acc: 0.7139 - val_loss: 0.9961 - val_acc: 0.7146\n",
      "Epoch 212/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9785 - acc: 0.7162 - val_loss: 0.9907 - val_acc: 0.7121\n",
      "Epoch 213/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9700 - acc: 0.7183 - val_loss: 1.0116 - val_acc: 0.7109\n",
      "Epoch 214/600\n",
      "1930/1930 [==============================] - 1s 581us/step - loss: 0.9817 - acc: 0.7147 - val_loss: 0.9880 - val_acc: 0.7199\n",
      "Epoch 215/600\n",
      "1930/1930 [==============================] - 1s 585us/step - loss: 0.9655 - acc: 0.7207 - val_loss: 0.9857 - val_acc: 0.7128\n",
      "Epoch 216/600\n",
      "1930/1930 [==============================] - 1s 580us/step - loss: 0.9583 - acc: 0.7207 - val_loss: 0.9802 - val_acc: 0.7193\n",
      "Epoch 217/600\n",
      "1930/1930 [==============================] - 1s 610us/step - loss: 0.9540 - acc: 0.7213 - val_loss: 0.9855 - val_acc: 0.7174\n",
      "Epoch 218/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.9621 - acc: 0.7210 - val_loss: 1.0297 - val_acc: 0.7012\n",
      "Epoch 219/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.9904 - acc: 0.7124 - val_loss: 1.0109 - val_acc: 0.7115\n",
      "Epoch 220/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 0.9723 - acc: 0.7181 - val_loss: 0.9840 - val_acc: 0.7187\n",
      "Epoch 221/600\n",
      "1930/1930 [==============================] - 1s 586us/step - loss: 0.9625 - acc: 0.7202 - val_loss: 0.9889 - val_acc: 0.7156\n",
      "Epoch 222/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9569 - acc: 0.7229 - val_loss: 0.9834 - val_acc: 0.7196\n",
      "Epoch 223/600\n",
      "1930/1930 [==============================] - 1s 570us/step - loss: 0.9571 - acc: 0.7217 - val_loss: 1.0628 - val_acc: 0.6860\n",
      "Epoch 224/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 0.9926 - acc: 0.7125 - val_loss: 1.0060 - val_acc: 0.7112\n",
      "Epoch 225/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9700 - acc: 0.7193 - val_loss: 1.0603 - val_acc: 0.6963\n",
      "Epoch 226/600\n",
      "1930/1930 [==============================] - 1s 583us/step - loss: 0.9881 - acc: 0.7120 - val_loss: 0.9971 - val_acc: 0.7159\n",
      "Epoch 227/600\n",
      "1930/1930 [==============================] - 1s 591us/step - loss: 0.9761 - acc: 0.7180 - val_loss: 0.9884 - val_acc: 0.7140\n",
      "Epoch 228/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.9547 - acc: 0.7236 - val_loss: 0.9850 - val_acc: 0.7137\n",
      "Epoch 229/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9472 - acc: 0.7247 - val_loss: 1.0116 - val_acc: 0.7087\n",
      "Epoch 230/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.9785 - acc: 0.7171 - val_loss: 0.9912 - val_acc: 0.7181\n",
      "Epoch 231/600\n",
      "1930/1930 [==============================] - 1s 579us/step - loss: 0.9546 - acc: 0.7238 - val_loss: 0.9792 - val_acc: 0.7165\n",
      "Epoch 232/600\n",
      "1930/1930 [==============================] - 1s 594us/step - loss: 0.9406 - acc: 0.7269 - val_loss: 0.9671 - val_acc: 0.7240\n",
      "Epoch 233/600\n",
      "1930/1930 [==============================] - 1s 577us/step - loss: 0.9371 - acc: 0.7265 - val_loss: 0.9956 - val_acc: 0.7128\n",
      "Epoch 234/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 0.9599 - acc: 0.7212 - val_loss: 0.9862 - val_acc: 0.7150\n",
      "Epoch 235/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 0.9452 - acc: 0.7249 - val_loss: 0.9737 - val_acc: 0.7212\n",
      "Epoch 236/600\n",
      "1930/1930 [==============================] - 1s 581us/step - loss: 0.9471 - acc: 0.7254 - val_loss: 0.9968 - val_acc: 0.7106\n",
      "Epoch 237/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930/1930 [==============================] - 1s 589us/step - loss: 0.9506 - acc: 0.7244 - val_loss: 0.9825 - val_acc: 0.7156\n",
      "Epoch 238/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.9435 - acc: 0.7268 - val_loss: 0.9664 - val_acc: 0.7206\n",
      "Epoch 239/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.9297 - acc: 0.7297 - val_loss: 0.9660 - val_acc: 0.7206\n",
      "Epoch 240/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.9312 - acc: 0.7285 - val_loss: 0.9967 - val_acc: 0.7115\n",
      "Epoch 241/600\n",
      "1930/1930 [==============================] - 1s 589us/step - loss: 0.9388 - acc: 0.7288 - val_loss: 0.9724 - val_acc: 0.7206\n",
      "Epoch 242/600\n",
      "1930/1930 [==============================] - 1s 595us/step - loss: 0.9280 - acc: 0.7304 - val_loss: 0.9679 - val_acc: 0.7218\n",
      "Epoch 243/600\n",
      "1930/1930 [==============================] - 1s 576us/step - loss: 0.9231 - acc: 0.7318 - val_loss: 0.9621 - val_acc: 0.7237\n",
      "Epoch 244/600\n",
      "1930/1930 [==============================] - 1s 579us/step - loss: 0.9203 - acc: 0.7333 - val_loss: 0.9682 - val_acc: 0.7196\n",
      "Epoch 245/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.9278 - acc: 0.7314 - val_loss: 0.9629 - val_acc: 0.7206\n",
      "Epoch 246/600\n",
      "1930/1930 [==============================] - 1s 576us/step - loss: 0.9235 - acc: 0.7336 - val_loss: 0.9604 - val_acc: 0.7240\n",
      "Epoch 247/600\n",
      "1930/1930 [==============================] - 1s 580us/step - loss: 0.9189 - acc: 0.7352 - val_loss: 0.9602 - val_acc: 0.7227\n",
      "Epoch 248/600\n",
      "1930/1930 [==============================] - 1s 585us/step - loss: 0.9151 - acc: 0.7339 - val_loss: 0.9816 - val_acc: 0.7121\n",
      "Epoch 249/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9236 - acc: 0.7318 - val_loss: 0.9669 - val_acc: 0.7227\n",
      "Epoch 250/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9222 - acc: 0.7341 - val_loss: 0.9839 - val_acc: 0.7187\n",
      "Epoch 251/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9303 - acc: 0.7316 - val_loss: 1.1321 - val_acc: 0.6872\n",
      "Epoch 252/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 1.0255 - acc: 0.7093 - val_loss: 1.0301 - val_acc: 0.7081\n",
      "Epoch 253/600\n",
      "1930/1930 [==============================] - 1s 591us/step - loss: 0.9561 - acc: 0.7277 - val_loss: 0.9964 - val_acc: 0.7168\n",
      "Epoch 254/600\n",
      "1930/1930 [==============================] - 1s 576us/step - loss: 0.9376 - acc: 0.7288 - val_loss: 0.9684 - val_acc: 0.7252\n",
      "Epoch 255/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9178 - acc: 0.7361 - val_loss: 0.9892 - val_acc: 0.7168\n",
      "Epoch 256/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.9344 - acc: 0.7307 - val_loss: 0.9763 - val_acc: 0.7196\n",
      "Epoch 257/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.9158 - acc: 0.7353 - val_loss: 0.9822 - val_acc: 0.7143\n",
      "Epoch 258/600\n",
      "1930/1930 [==============================] - 1s 589us/step - loss: 0.9103 - acc: 0.7366 - val_loss: 1.0075 - val_acc: 0.7118\n",
      "Epoch 259/600\n",
      "1930/1930 [==============================] - 1s 614us/step - loss: 0.9376 - acc: 0.7290 - val_loss: 0.9885 - val_acc: 0.7187\n",
      "Epoch 260/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 0.9144 - acc: 0.7375 - val_loss: 0.9582 - val_acc: 0.7237\n",
      "Epoch 261/600\n",
      "1930/1930 [==============================] - 1s 717us/step - loss: 0.9227 - acc: 0.7373 - val_loss: 0.9470 - val_acc: 0.7333\n",
      "Epoch 262/600\n",
      "1930/1930 [==============================] - 1s 659us/step - loss: 0.9104 - acc: 0.7374 - val_loss: 0.9632 - val_acc: 0.7237\n",
      "Epoch 263/600\n",
      "1930/1930 [==============================] - 1s 617us/step - loss: 0.8994 - acc: 0.7408 - val_loss: 0.9743 - val_acc: 0.7184\n",
      "Epoch 264/600\n",
      "1930/1930 [==============================] - 1s 602us/step - loss: 0.9056 - acc: 0.7396 - val_loss: 0.9503 - val_acc: 0.7271\n",
      "Epoch 265/600\n",
      "1930/1930 [==============================] - 1s 576us/step - loss: 0.8888 - acc: 0.7447 - val_loss: 0.9536 - val_acc: 0.7249\n",
      "Epoch 266/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.8927 - acc: 0.7413 - val_loss: 0.9607 - val_acc: 0.7237\n",
      "Epoch 267/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 0.8995 - acc: 0.7419 - val_loss: 0.9475 - val_acc: 0.7312\n",
      "Epoch 268/600\n",
      "1930/1930 [==============================] - 1s 591us/step - loss: 0.8917 - acc: 0.7447 - val_loss: 0.9684 - val_acc: 0.7231\n",
      "Epoch 269/600\n",
      "1930/1930 [==============================] - 1s 591us/step - loss: 0.9118 - acc: 0.7380 - val_loss: 0.9768 - val_acc: 0.7196\n",
      "Epoch 270/600\n",
      "1930/1930 [==============================] - 1s 576us/step - loss: 0.9163 - acc: 0.7368 - val_loss: 0.9670 - val_acc: 0.7268\n",
      "Epoch 271/600\n",
      "1930/1930 [==============================] - 1s 577us/step - loss: 0.8925 - acc: 0.7440 - val_loss: 0.9442 - val_acc: 0.7249\n",
      "Epoch 272/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.8820 - acc: 0.7472 - val_loss: 1.0416 - val_acc: 0.7044\n",
      "Epoch 273/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.9783 - acc: 0.7234 - val_loss: 0.9870 - val_acc: 0.7209\n",
      "Epoch 274/600\n",
      "1930/1930 [==============================] - 1s 587us/step - loss: 0.9175 - acc: 0.7401 - val_loss: 0.9979 - val_acc: 0.7199\n",
      "Epoch 275/600\n",
      "1930/1930 [==============================] - 1s 586us/step - loss: 0.9235 - acc: 0.7384 - val_loss: 0.9852 - val_acc: 0.7187\n",
      "Epoch 276/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9119 - acc: 0.7423 - val_loss: 0.9467 - val_acc: 0.7299\n",
      "Epoch 277/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.8714 - acc: 0.7515 - val_loss: 0.9531 - val_acc: 0.7274\n",
      "Epoch 278/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 0.8956 - acc: 0.7420 - val_loss: 0.9345 - val_acc: 0.7352\n",
      "Epoch 279/600\n",
      "1930/1930 [==============================] - 1s 577us/step - loss: 0.8729 - acc: 0.7506 - val_loss: 0.9360 - val_acc: 0.7343\n",
      "Epoch 280/600\n",
      "1930/1930 [==============================] - 1s 606us/step - loss: 0.8671 - acc: 0.7507 - val_loss: 0.9223 - val_acc: 0.7330\n",
      "Epoch 281/600\n",
      "1930/1930 [==============================] - 1s 581us/step - loss: 0.8572 - acc: 0.7536 - val_loss: 0.9312 - val_acc: 0.7327\n",
      "Epoch 282/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.8617 - acc: 0.7539 - val_loss: 0.9319 - val_acc: 0.7343\n",
      "Epoch 283/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.8606 - acc: 0.7547 - val_loss: 0.9343 - val_acc: 0.7343\n",
      "Epoch 284/600\n",
      "1930/1930 [==============================] - 1s 577us/step - loss: 0.8615 - acc: 0.7527 - val_loss: 0.9508 - val_acc: 0.7302\n",
      "Epoch 285/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.8674 - acc: 0.7523 - val_loss: 0.9237 - val_acc: 0.7402\n",
      "Epoch 286/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.8528 - acc: 0.7556 - val_loss: 0.9396 - val_acc: 0.7296\n",
      "Epoch 287/600\n",
      "1930/1930 [==============================] - 1s 588us/step - loss: 0.8515 - acc: 0.7564 - val_loss: 0.9190 - val_acc: 0.7361\n",
      "Epoch 288/600\n",
      "1930/1930 [==============================] - 1s 595us/step - loss: 0.8528 - acc: 0.7547 - val_loss: 0.9199 - val_acc: 0.7380\n",
      "Epoch 289/600\n",
      "1930/1930 [==============================] - 1s 577us/step - loss: 0.8469 - acc: 0.7590 - val_loss: 0.9226 - val_acc: 0.7389\n",
      "Epoch 290/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.8446 - acc: 0.7586 - val_loss: 0.9704 - val_acc: 0.7268\n",
      "Epoch 291/600\n",
      "1930/1930 [==============================] - 1s 570us/step - loss: 0.8768 - acc: 0.7493 - val_loss: 1.0667 - val_acc: 0.7000\n",
      "Epoch 292/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.9689 - acc: 0.7247 - val_loss: 0.9968 - val_acc: 0.7196\n",
      "Epoch 293/600\n",
      "1930/1930 [==============================] - 1s 576us/step - loss: 0.8875 - acc: 0.7493 - val_loss: 0.9286 - val_acc: 0.7399\n",
      "Epoch 294/600\n",
      "1930/1930 [==============================] - 1s 600us/step - loss: 0.8467 - acc: 0.7600 - val_loss: 0.9105 - val_acc: 0.7380\n",
      "Epoch 295/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.8407 - acc: 0.7585 - val_loss: 0.9253 - val_acc: 0.7393\n",
      "Epoch 296/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.8467 - acc: 0.7583 - val_loss: 0.9201 - val_acc: 0.7364\n",
      "Epoch 297/600\n",
      "1930/1930 [==============================] - 1s 570us/step - loss: 0.8371 - acc: 0.7613 - val_loss: 0.9301 - val_acc: 0.7368\n",
      "Epoch 298/600\n",
      "1930/1930 [==============================] - 1s 576us/step - loss: 0.8330 - acc: 0.7630 - val_loss: 0.9332 - val_acc: 0.7312\n",
      "Epoch 299/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.8352 - acc: 0.7637 - val_loss: 0.9068 - val_acc: 0.7439\n",
      "Epoch 300/600\n",
      "1930/1930 [==============================] - 1s 589us/step - loss: 0.8238 - acc: 0.7656 - val_loss: 0.9358 - val_acc: 0.7346\n",
      "Epoch 301/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 0.8528 - acc: 0.7554 - val_loss: 0.9123 - val_acc: 0.7389\n",
      "Epoch 302/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.8414 - acc: 0.7589 - val_loss: 0.9292 - val_acc: 0.7389\n",
      "Epoch 303/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.8449 - acc: 0.7598 - val_loss: 0.9553 - val_acc: 0.7330\n",
      "Epoch 304/600\n",
      "1930/1930 [==============================] - 1s 569us/step - loss: 0.8400 - acc: 0.7622 - val_loss: 0.9239 - val_acc: 0.7389\n",
      "Epoch 305/600\n",
      "1930/1930 [==============================] - 1s 586us/step - loss: 0.8292 - acc: 0.7637 - val_loss: 0.9115 - val_acc: 0.7436\n",
      "Epoch 306/600\n",
      "1930/1930 [==============================] - 1s 586us/step - loss: 0.8176 - acc: 0.7665 - val_loss: 0.9095 - val_acc: 0.7436\n",
      "Epoch 307/600\n",
      "1930/1930 [==============================] - 1s 578us/step - loss: 0.8176 - acc: 0.7668 - val_loss: 0.9023 - val_acc: 0.7458\n",
      "Epoch 308/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.8181 - acc: 0.7665 - val_loss: 0.9156 - val_acc: 0.7417\n",
      "Epoch 309/600\n",
      "1930/1930 [==============================] - 1s 569us/step - loss: 0.8184 - acc: 0.7673 - val_loss: 0.9226 - val_acc: 0.7427\n",
      "Epoch 310/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.8214 - acc: 0.7645 - val_loss: 0.9302 - val_acc: 0.7424\n",
      "Epoch 311/600\n",
      "1930/1930 [==============================] - 1s 584us/step - loss: 0.8188 - acc: 0.7660 - val_loss: 0.9407 - val_acc: 0.7358\n",
      "Epoch 312/600\n",
      "1930/1930 [==============================] - 1s 580us/step - loss: 0.8272 - acc: 0.7646 - val_loss: 0.9460 - val_acc: 0.7333\n",
      "Epoch 313/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.8395 - acc: 0.7620 - val_loss: 0.9303 - val_acc: 0.7424\n",
      "Epoch 314/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.8190 - acc: 0.7668 - val_loss: 1.3456 - val_acc: 0.6526\n",
      "Epoch 315/600\n",
      "1930/1930 [==============================] - 1s 641us/step - loss: 1.2198 - acc: 0.6672 - val_loss: 1.1120 - val_acc: 0.6894\n",
      "Epoch 316/600\n",
      "1930/1930 [==============================] - 1s 604us/step - loss: 0.9851 - acc: 0.7237 - val_loss: 0.9764 - val_acc: 0.7259\n",
      "Epoch 317/600\n",
      "1930/1930 [==============================] - 1s 593us/step - loss: 0.8879 - acc: 0.7493 - val_loss: 0.9174 - val_acc: 0.7414\n",
      "Epoch 318/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.8379 - acc: 0.7623 - val_loss: 0.9172 - val_acc: 0.7386\n",
      "Epoch 319/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.8271 - acc: 0.7648 - val_loss: 0.9180 - val_acc: 0.7380\n",
      "Epoch 320/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.8211 - acc: 0.7672 - val_loss: 0.9328 - val_acc: 0.7371\n",
      "Epoch 321/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.8301 - acc: 0.7639 - val_loss: 0.9138 - val_acc: 0.7430\n",
      "Epoch 322/600\n",
      "1930/1930 [==============================] - 1s 582us/step - loss: 0.8183 - acc: 0.7691 - val_loss: 0.9086 - val_acc: 0.7408\n",
      "Epoch 323/600\n",
      "1930/1930 [==============================] - 1s 585us/step - loss: 0.8016 - acc: 0.7714 - val_loss: 0.8870 - val_acc: 0.7452\n",
      "Epoch 324/600\n",
      "1930/1930 [==============================] - 1s 577us/step - loss: 0.7906 - acc: 0.7761 - val_loss: 0.8877 - val_acc: 0.7520\n",
      "Epoch 325/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.7916 - acc: 0.7752 - val_loss: 0.9102 - val_acc: 0.7417\n",
      "Epoch 326/600\n",
      "1930/1930 [==============================] - 1s 574us/step - loss: 0.8060 - acc: 0.7739 - val_loss: 0.9086 - val_acc: 0.7464\n",
      "Epoch 327/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.7993 - acc: 0.7729 - val_loss: 0.9003 - val_acc: 0.7449\n",
      "Epoch 328/600\n",
      "1930/1930 [==============================] - 1s 588us/step - loss: 0.7948 - acc: 0.7758 - val_loss: 0.9529 - val_acc: 0.7283\n",
      "Epoch 329/600\n",
      "1930/1930 [==============================] - 1s 584us/step - loss: 0.7919 - acc: 0.7753 - val_loss: 0.8891 - val_acc: 0.7474\n",
      "Epoch 330/600\n",
      "1930/1930 [==============================] - 1s 569us/step - loss: 0.7760 - acc: 0.7798 - val_loss: 0.8930 - val_acc: 0.7502\n",
      "Epoch 331/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.7819 - acc: 0.7806 - val_loss: 0.8874 - val_acc: 0.7517\n",
      "Epoch 332/600\n",
      "1930/1930 [==============================] - 1s 572us/step - loss: 0.7834 - acc: 0.7792 - val_loss: 0.8837 - val_acc: 0.7511\n",
      "Epoch 333/600\n",
      "1930/1930 [==============================] - 1s 584us/step - loss: 0.7729 - acc: 0.7815 - val_loss: 0.8912 - val_acc: 0.7489\n",
      "Epoch 334/600\n",
      "1930/1930 [==============================] - 1s 590us/step - loss: 0.7673 - acc: 0.7841 - val_loss: 0.8839 - val_acc: 0.7498\n",
      "Epoch 335/600\n",
      "1930/1930 [==============================] - 1s 575us/step - loss: 0.7626 - acc: 0.7863 - val_loss: 0.8763 - val_acc: 0.7502\n",
      "Epoch 336/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.7607 - acc: 0.7850 - val_loss: 0.8763 - val_acc: 0.7520\n",
      "Epoch 337/600\n",
      "1930/1930 [==============================] - 1s 573us/step - loss: 0.7509 - acc: 0.7874 - val_loss: 0.8708 - val_acc: 0.7533\n",
      "Epoch 338/600\n",
      "1930/1930 [==============================] - 1s 571us/step - loss: 0.7628 - acc: 0.7845 - val_loss: 0.8795 - val_acc: 0.7558\n",
      "Epoch 339/600\n",
      "1930/1930 [==============================] - 1s 592us/step - loss: 0.7730 - acc: 0.7828 - val_loss: 0.9025 - val_acc: 0.7470\n",
      "Epoch 340/600\n",
      "1930/1930 [==============================] - 1s 585us/step - loss: 0.7651 - acc: 0.7844 - val_loss: 0.9977 - val_acc: 0.7202\n",
      "Epoch 341/600\n",
      "1930/1930 [==============================] - 1s 584us/step - loss: 0.8563 - acc: 0.7622 - val_loss: 0.9294 - val_acc: 0.7414\n",
      "Epoch 342/600\n",
      "1930/1930 [==============================] - 1s 617us/step - loss: 0.7996 - acc: 0.7778 - val_loss: 0.8950 - val_acc: 0.7536\n",
      "Epoch 343/600\n",
      "1930/1930 [==============================] - 1s 671us/step - loss: 0.7570 - acc: 0.7884 - val_loss: 0.9065 - val_acc: 0.7445\n",
      "Epoch 344/600\n",
      "1930/1930 [==============================] - 2s 805us/step - loss: 0.7567 - acc: 0.7868 - val_loss: 0.8830 - val_acc: 0.7555\n",
      "Epoch 345/600\n",
      "1930/1930 [==============================] - 2s 805us/step - loss: 0.7509 - acc: 0.7900 - val_loss: 0.8734 - val_acc: 0.7589\n",
      "Epoch 346/600\n",
      "1930/1930 [==============================] - 1s 766us/step - loss: 0.7371 - acc: 0.7929 - val_loss: 0.8760 - val_acc: 0.7536\n",
      "Epoch 347/600\n",
      "1930/1930 [==============================] - 1s 689us/step - loss: 0.7319 - acc: 0.7951 - val_loss: 0.8737 - val_acc: 0.7517\n",
      "Epoch 348/600\n",
      "1930/1930 [==============================] - 1s 690us/step - loss: 0.7283 - acc: 0.7949 - val_loss: 0.8797 - val_acc: 0.7567\n",
      "Epoch 349/600\n",
      "1930/1930 [==============================] - 1s 720us/step - loss: 0.7322 - acc: 0.7949 - val_loss: 0.8787 - val_acc: 0.7551\n",
      "Epoch 350/600\n",
      "1930/1930 [==============================] - 1s 704us/step - loss: 0.7301 - acc: 0.7935 - val_loss: 0.8964 - val_acc: 0.7530\n",
      "Epoch 351/600\n",
      "1930/1930 [==============================] - 1s 703us/step - loss: 0.7402 - acc: 0.7946 - val_loss: 0.9860 - val_acc: 0.7221\n",
      "Epoch 352/600\n",
      "1930/1930 [==============================] - 1s 688us/step - loss: 0.7757 - acc: 0.7820 - val_loss: 0.9011 - val_acc: 0.7502\n",
      "Epoch 353/600\n",
      "1930/1930 [==============================] - 1s 691us/step - loss: 0.7682 - acc: 0.7851 - val_loss: 0.8856 - val_acc: 0.7508\n",
      "Epoch 354/600\n",
      "1930/1930 [==============================] - 1s 680us/step - loss: 0.7853 - acc: 0.7801 - val_loss: 0.9435 - val_acc: 0.7383\n",
      "Epoch 355/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930/1930 [==============================] - 1s 667us/step - loss: 0.7701 - acc: 0.7845 - val_loss: 0.8956 - val_acc: 0.7517\n",
      "Epoch 356/600\n",
      "1930/1930 [==============================] - 1s 662us/step - loss: 0.7391 - acc: 0.7935 - val_loss: 0.8583 - val_acc: 0.7611\n",
      "Epoch 357/600\n",
      "1930/1930 [==============================] - 1s 689us/step - loss: 0.7336 - acc: 0.7937 - val_loss: 0.8610 - val_acc: 0.7607\n",
      "Epoch 358/600\n",
      "1930/1930 [==============================] - 1s 624us/step - loss: 0.7189 - acc: 0.8004 - val_loss: 0.8520 - val_acc: 0.7626\n",
      "Epoch 359/600\n",
      "1930/1930 [==============================] - 1s 639us/step - loss: 0.7128 - acc: 0.8014 - val_loss: 0.8641 - val_acc: 0.7570\n",
      "Epoch 360/600\n",
      "1930/1930 [==============================] - 1s 711us/step - loss: 0.7169 - acc: 0.8000 - val_loss: 0.8660 - val_acc: 0.7579\n",
      "Epoch 361/600\n",
      "1930/1930 [==============================] - 1s 710us/step - loss: 0.7030 - acc: 0.8044 - val_loss: 0.8365 - val_acc: 0.7673\n",
      "Epoch 362/600\n",
      "1930/1930 [==============================] - 1s 676us/step - loss: 0.6922 - acc: 0.8072 - val_loss: 0.8507 - val_acc: 0.7586\n",
      "Epoch 363/600\n",
      "1930/1930 [==============================] - 1s 668us/step - loss: 0.7114 - acc: 0.8009 - val_loss: 0.8751 - val_acc: 0.7523\n",
      "Epoch 364/600\n",
      "1930/1930 [==============================] - 1s 705us/step - loss: 0.7069 - acc: 0.8034 - val_loss: 0.8490 - val_acc: 0.7589\n",
      "Epoch 365/600\n",
      "1930/1930 [==============================] - 1s 695us/step - loss: 0.7011 - acc: 0.8055 - val_loss: 0.8585 - val_acc: 0.7611\n",
      "Epoch 366/600\n",
      "1930/1930 [==============================] - 1s 661us/step - loss: 0.7033 - acc: 0.8035 - val_loss: 0.8906 - val_acc: 0.7517\n",
      "Epoch 367/600\n",
      "1930/1930 [==============================] - 1s 657us/step - loss: 0.7035 - acc: 0.8049 - val_loss: 0.8603 - val_acc: 0.7632\n",
      "Epoch 368/600\n",
      "1930/1930 [==============================] - 1s 639us/step - loss: 0.6967 - acc: 0.8058 - val_loss: 0.8673 - val_acc: 0.7595\n",
      "Epoch 369/600\n",
      "1930/1930 [==============================] - 1s 667us/step - loss: 0.6925 - acc: 0.8072 - val_loss: 0.8630 - val_acc: 0.7576\n",
      "Epoch 370/600\n",
      "1930/1930 [==============================] - 1s 725us/step - loss: 0.6905 - acc: 0.8075 - val_loss: 0.9449 - val_acc: 0.7408\n",
      "Epoch 371/600\n",
      "1930/1930 [==============================] - 1s 665us/step - loss: 0.8066 - acc: 0.7782 - val_loss: 0.9121 - val_acc: 0.7514\n",
      "Epoch 372/600\n",
      "1930/1930 [==============================] - 1s 632us/step - loss: 0.8018 - acc: 0.7803 - val_loss: 0.9015 - val_acc: 0.7517\n",
      "Epoch 373/600\n",
      "1930/1930 [==============================] - 1s 666us/step - loss: 0.7375 - acc: 0.7946 - val_loss: 0.8732 - val_acc: 0.7555\n",
      "Epoch 374/600\n",
      "1930/1930 [==============================] - 1s 641us/step - loss: 0.6915 - acc: 0.8077 - val_loss: 0.8452 - val_acc: 0.7645\n",
      "Epoch 375/600\n",
      "1930/1930 [==============================] - 1s 640us/step - loss: 0.6735 - acc: 0.8121 - val_loss: 0.8383 - val_acc: 0.7648\n",
      "Epoch 376/600\n",
      "1930/1930 [==============================] - 1s 632us/step - loss: 0.6728 - acc: 0.8131 - val_loss: 0.8324 - val_acc: 0.7679\n",
      "Epoch 377/600\n",
      "1930/1930 [==============================] - 1s 631us/step - loss: 0.6605 - acc: 0.8156 - val_loss: 0.8276 - val_acc: 0.7685\n",
      "Epoch 378/600\n",
      "1930/1930 [==============================] - 1s 646us/step - loss: 0.6614 - acc: 0.8153 - val_loss: 0.8493 - val_acc: 0.7654\n",
      "Epoch 379/600\n",
      "1930/1930 [==============================] - 1s 646us/step - loss: 0.6723 - acc: 0.8124 - val_loss: 0.8614 - val_acc: 0.7607\n",
      "Epoch 380/600\n",
      "1930/1930 [==============================] - 1s 686us/step - loss: 0.6784 - acc: 0.8109 - val_loss: 0.8248 - val_acc: 0.7685\n",
      "Epoch 381/600\n",
      "1930/1930 [==============================] - 1s 665us/step - loss: 0.6546 - acc: 0.8188 - val_loss: 0.8368 - val_acc: 0.7679\n",
      "Epoch 382/600\n",
      "1930/1930 [==============================] - 1s 677us/step - loss: 0.6565 - acc: 0.8185 - val_loss: 0.8205 - val_acc: 0.7729\n",
      "Epoch 383/600\n",
      "1930/1930 [==============================] - 1s 651us/step - loss: 0.6496 - acc: 0.8196 - val_loss: 0.8185 - val_acc: 0.7745\n",
      "Epoch 384/600\n",
      "1930/1930 [==============================] - 1s 677us/step - loss: 0.6397 - acc: 0.8228 - val_loss: 0.8304 - val_acc: 0.7673\n",
      "Epoch 385/600\n",
      "1930/1930 [==============================] - 1s 765us/step - loss: 0.6418 - acc: 0.8221 - val_loss: 0.8357 - val_acc: 0.7704\n",
      "Epoch 386/600\n",
      "1930/1930 [==============================] - 1s 730us/step - loss: 0.6513 - acc: 0.8179 - val_loss: 0.8372 - val_acc: 0.7651\n",
      "Epoch 387/600\n",
      "1930/1930 [==============================] - 1s 698us/step - loss: 0.6873 - acc: 0.8089 - val_loss: 0.8392 - val_acc: 0.7707\n",
      "Epoch 388/600\n",
      "1930/1930 [==============================] - 1s 689us/step - loss: 0.6828 - acc: 0.8101 - val_loss: 0.8432 - val_acc: 0.7679\n",
      "Epoch 389/600\n",
      "1930/1930 [==============================] - 1s 648us/step - loss: 0.6552 - acc: 0.8204 - val_loss: 0.8579 - val_acc: 0.7598\n",
      "Epoch 390/600\n",
      "1930/1930 [==============================] - 1s 748us/step - loss: 0.6602 - acc: 0.8176 - val_loss: 0.8712 - val_acc: 0.7601\n",
      "Epoch 391/600\n",
      "1930/1930 [==============================] - 1s 696us/step - loss: 0.6700 - acc: 0.8152 - val_loss: 0.8399 - val_acc: 0.7707\n",
      "Epoch 392/600\n",
      "1930/1930 [==============================] - 2s 828us/step - loss: 0.6532 - acc: 0.8181 - val_loss: 0.8272 - val_acc: 0.7723\n",
      "Epoch 393/600\n",
      "1930/1930 [==============================] - 1s 771us/step - loss: 0.6449 - acc: 0.8228 - val_loss: 0.8408 - val_acc: 0.7657\n",
      "Epoch 394/600\n",
      "1930/1930 [==============================] - 2s 802us/step - loss: 0.6483 - acc: 0.8209 - val_loss: 0.8324 - val_acc: 0.7707\n",
      "Epoch 395/600\n",
      "1930/1930 [==============================] - 1s 716us/step - loss: 0.6354 - acc: 0.8235 - val_loss: 0.8304 - val_acc: 0.7710\n",
      "Epoch 396/600\n",
      "1930/1930 [==============================] - 1s 665us/step - loss: 0.6338 - acc: 0.8249 - val_loss: 0.9455 - val_acc: 0.7396\n",
      "Epoch 397/600\n",
      "1930/1930 [==============================] - 1s 631us/step - loss: 0.8604 - acc: 0.7683 - val_loss: 0.9694 - val_acc: 0.7371\n",
      "Epoch 398/600\n",
      "1930/1930 [==============================] - 1s 634us/step - loss: 0.7474 - acc: 0.7967 - val_loss: 0.8702 - val_acc: 0.7589\n",
      "Epoch 399/600\n",
      "1930/1930 [==============================] - 1s 645us/step - loss: 0.6725 - acc: 0.8152 - val_loss: 0.8196 - val_acc: 0.7732\n",
      "Epoch 400/600\n",
      "1930/1930 [==============================] - 1s 642us/step - loss: 0.6299 - acc: 0.8251 - val_loss: 0.8455 - val_acc: 0.7604\n",
      "Epoch 401/600\n",
      "1930/1930 [==============================] - 1s 629us/step - loss: 0.6269 - acc: 0.8251 - val_loss: 0.8300 - val_acc: 0.7704\n",
      "Epoch 402/600\n",
      "1930/1930 [==============================] - 1s 630us/step - loss: 0.6269 - acc: 0.8272 - val_loss: 0.8237 - val_acc: 0.7726\n",
      "Epoch 403/600\n",
      "1930/1930 [==============================] - 1s 647us/step - loss: 0.6165 - acc: 0.8290 - val_loss: 0.8126 - val_acc: 0.7723\n",
      "Epoch 404/600\n",
      "1930/1930 [==============================] - 1s 646us/step - loss: 0.6185 - acc: 0.8299 - val_loss: 0.8270 - val_acc: 0.7704\n",
      "Epoch 405/600\n",
      "1930/1930 [==============================] - 1s 634us/step - loss: 0.6200 - acc: 0.8271 - val_loss: 0.8206 - val_acc: 0.7720\n",
      "Epoch 406/600\n",
      "1930/1930 [==============================] - 1s 636us/step - loss: 0.6195 - acc: 0.8271 - val_loss: 0.8222 - val_acc: 0.7757\n",
      "Epoch 407/600\n",
      "1930/1930 [==============================] - 1s 646us/step - loss: 0.6057 - acc: 0.8340 - val_loss: 0.8113 - val_acc: 0.7794\n",
      "Epoch 408/600\n",
      "1930/1930 [==============================] - 1s 650us/step - loss: 0.6036 - acc: 0.8335 - val_loss: 0.8258 - val_acc: 0.7754\n",
      "Epoch 409/600\n",
      "1930/1930 [==============================] - 1s 639us/step - loss: 0.6092 - acc: 0.8318 - val_loss: 0.8487 - val_acc: 0.7701\n",
      "Epoch 410/600\n",
      "1930/1930 [==============================] - 1s 677us/step - loss: 0.6182 - acc: 0.8293 - val_loss: 0.8457 - val_acc: 0.7651\n",
      "Epoch 411/600\n",
      "1930/1930 [==============================] - 1s 680us/step - loss: 0.6104 - acc: 0.8301 - val_loss: 0.8124 - val_acc: 0.7735\n",
      "Epoch 412/600\n",
      "1930/1930 [==============================] - 1s 659us/step - loss: 0.5972 - acc: 0.8386 - val_loss: 0.8301 - val_acc: 0.7726\n",
      "Epoch 413/600\n",
      "1930/1930 [==============================] - 1s 636us/step - loss: 0.6091 - acc: 0.8321 - val_loss: 0.8352 - val_acc: 0.7754\n",
      "Epoch 414/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930/1930 [==============================] - 1s 613us/step - loss: 0.6815 - acc: 0.8117 - val_loss: 0.9299 - val_acc: 0.7474\n",
      "Epoch 415/600\n",
      "1930/1930 [==============================] - 1s 610us/step - loss: 0.7463 - acc: 0.7960 - val_loss: 0.8846 - val_acc: 0.7601\n",
      "Epoch 416/600\n",
      "1930/1930 [==============================] - 1s 623us/step - loss: 0.6482 - acc: 0.8249 - val_loss: 0.8379 - val_acc: 0.7692\n",
      "Epoch 417/600\n",
      "1930/1930 [==============================] - 1s 675us/step - loss: 0.6199 - acc: 0.8314 - val_loss: 0.8229 - val_acc: 0.7757\n",
      "Epoch 418/600\n",
      "1930/1930 [==============================] - 1s 618us/step - loss: 0.5880 - acc: 0.8393 - val_loss: 0.8267 - val_acc: 0.7701\n",
      "Epoch 419/600\n",
      "1930/1930 [==============================] - 1s 622us/step - loss: 0.5880 - acc: 0.8398 - val_loss: 0.8334 - val_acc: 0.7692\n",
      "Epoch 420/600\n",
      "1930/1930 [==============================] - 1s 640us/step - loss: 0.5874 - acc: 0.8393 - val_loss: 0.8284 - val_acc: 0.7685\n",
      "Epoch 421/600\n",
      "1930/1930 [==============================] - 1s 748us/step - loss: 0.5947 - acc: 0.8366 - val_loss: 0.8333 - val_acc: 0.7738\n",
      "Epoch 422/600\n",
      "1930/1930 [==============================] - 1s 631us/step - loss: 0.5859 - acc: 0.8400 - val_loss: 0.8130 - val_acc: 0.7788\n",
      "Epoch 423/600\n",
      "1930/1930 [==============================] - 1s 615us/step - loss: 0.5742 - acc: 0.8447 - val_loss: 0.8085 - val_acc: 0.7776\n",
      "Epoch 424/600\n",
      "1930/1930 [==============================] - 1s 619us/step - loss: 0.5674 - acc: 0.8446 - val_loss: 0.8087 - val_acc: 0.7769\n",
      "Epoch 425/600\n",
      "1930/1930 [==============================] - 1s 635us/step - loss: 0.5683 - acc: 0.8446 - val_loss: 0.8224 - val_acc: 0.7754\n",
      "Epoch 426/600\n",
      "1930/1930 [==============================] - 1s 632us/step - loss: 0.5758 - acc: 0.8453 - val_loss: 0.8252 - val_acc: 0.7760\n",
      "Epoch 427/600\n",
      "1930/1930 [==============================] - 1s 627us/step - loss: 0.5741 - acc: 0.8432 - val_loss: 0.8268 - val_acc: 0.7779\n",
      "Epoch 428/600\n",
      "1930/1930 [==============================] - 1s 626us/step - loss: 0.5741 - acc: 0.8416 - val_loss: 0.8301 - val_acc: 0.7751\n",
      "Epoch 429/600\n",
      "1930/1930 [==============================] - 1s 633us/step - loss: 0.5848 - acc: 0.8407 - val_loss: 1.0267 - val_acc: 0.7255\n",
      "Epoch 430/600\n",
      "1930/1930 [==============================] - 1s 634us/step - loss: 0.7429 - acc: 0.7980 - val_loss: 0.8881 - val_acc: 0.7626\n",
      "Epoch 431/600\n",
      "1930/1930 [==============================] - 1s 620us/step - loss: 0.6661 - acc: 0.8190 - val_loss: 0.8318 - val_acc: 0.7776\n",
      "Epoch 432/600\n",
      "1930/1930 [==============================] - 1s 623us/step - loss: 0.6012 - acc: 0.8375 - val_loss: 0.8196 - val_acc: 0.7813\n",
      "Epoch 433/600\n",
      "1930/1930 [==============================] - 1s 625us/step - loss: 0.5778 - acc: 0.8429 - val_loss: 0.8216 - val_acc: 0.7776\n",
      "Epoch 434/600\n",
      "1930/1930 [==============================] - 1s 651us/step - loss: 0.5596 - acc: 0.8471 - val_loss: 0.8076 - val_acc: 0.7807\n",
      "Epoch 435/600\n",
      "1930/1930 [==============================] - 1s 641us/step - loss: 0.5715 - acc: 0.8431 - val_loss: 0.8168 - val_acc: 0.7751\n",
      "Epoch 436/600\n",
      "1930/1930 [==============================] - 1s 687us/step - loss: 0.5530 - acc: 0.8490 - val_loss: 0.8207 - val_acc: 0.7760\n",
      "Epoch 437/600\n",
      "1930/1930 [==============================] - 1s 668us/step - loss: 0.5617 - acc: 0.8466 - val_loss: 0.8191 - val_acc: 0.7813\n",
      "Epoch 438/600\n",
      "1930/1930 [==============================] - 1s 653us/step - loss: 0.5657 - acc: 0.8447 - val_loss: 0.8251 - val_acc: 0.7782\n",
      "Epoch 439/600\n",
      "1930/1930 [==============================] - 1s 695us/step - loss: 0.5586 - acc: 0.8481 - val_loss: 0.8066 - val_acc: 0.7788\n",
      "Epoch 440/600\n",
      "1930/1930 [==============================] - 1s 759us/step - loss: 0.5528 - acc: 0.8493 - val_loss: 0.8160 - val_acc: 0.7732\n",
      "Epoch 441/600\n",
      "1930/1930 [==============================] - 1s 744us/step - loss: 0.5521 - acc: 0.8506 - val_loss: 0.8017 - val_acc: 0.7822\n",
      "Epoch 442/600\n",
      "1930/1930 [==============================] - 1s 726us/step - loss: 0.5383 - acc: 0.8538 - val_loss: 0.8000 - val_acc: 0.7816\n",
      "Epoch 443/600\n",
      "1930/1930 [==============================] - 1s 773us/step - loss: 0.5461 - acc: 0.8513 - val_loss: 0.8502 - val_acc: 0.7698\n",
      "Epoch 444/600\n",
      "1930/1930 [==============================] - 1s 720us/step - loss: 0.5743 - acc: 0.8404 - val_loss: 0.8069 - val_acc: 0.7850\n",
      "Epoch 445/600\n",
      "1930/1930 [==============================] - 1s 677us/step - loss: 0.5681 - acc: 0.8456 - val_loss: 0.8238 - val_acc: 0.7769\n",
      "Epoch 446/600\n",
      "1930/1930 [==============================] - 1s 737us/step - loss: 0.5488 - acc: 0.8539 - val_loss: 0.8217 - val_acc: 0.7776\n",
      "Epoch 447/600\n",
      "1930/1930 [==============================] - 1s 757us/step - loss: 0.5377 - acc: 0.8542 - val_loss: 0.8441 - val_acc: 0.7688\n",
      "Epoch 448/600\n",
      "1930/1930 [==============================] - 1s 771us/step - loss: 0.5428 - acc: 0.8539 - val_loss: 0.8130 - val_acc: 0.7791\n",
      "Epoch 449/600\n",
      "1930/1930 [==============================] - 1s 754us/step - loss: 0.5252 - acc: 0.8597 - val_loss: 0.7947 - val_acc: 0.7869\n",
      "Epoch 450/600\n",
      "1930/1930 [==============================] - 1s 740us/step - loss: 0.5210 - acc: 0.8577 - val_loss: 0.8053 - val_acc: 0.7841\n",
      "Epoch 451/600\n",
      "1930/1930 [==============================] - 1s 691us/step - loss: 0.5155 - acc: 0.8616 - val_loss: 0.8136 - val_acc: 0.7832\n",
      "Epoch 452/600\n",
      "1930/1930 [==============================] - 2s 784us/step - loss: 0.5099 - acc: 0.8628 - val_loss: 0.8196 - val_acc: 0.7779\n",
      "Epoch 453/600\n",
      "1930/1930 [==============================] - 1s 756us/step - loss: 0.5136 - acc: 0.8609 - val_loss: 0.8225 - val_acc: 0.7807\n",
      "Epoch 454/600\n",
      "1930/1930 [==============================] - 1s 737us/step - loss: 0.5234 - acc: 0.8573 - val_loss: 0.8189 - val_acc: 0.7773\n",
      "Epoch 455/600\n",
      "1930/1930 [==============================] - 1s 764us/step - loss: 0.5300 - acc: 0.8568 - val_loss: 0.8209 - val_acc: 0.7763\n",
      "Epoch 456/600\n",
      "1930/1930 [==============================] - 2s 795us/step - loss: 0.5197 - acc: 0.8598 - val_loss: 0.8283 - val_acc: 0.7769\n",
      "Epoch 457/600\n",
      "1930/1930 [==============================] - 1s 760us/step - loss: 0.5196 - acc: 0.8601 - val_loss: 0.8202 - val_acc: 0.7832\n",
      "Epoch 458/600\n",
      "1930/1930 [==============================] - 1s 720us/step - loss: 0.5230 - acc: 0.8589 - val_loss: 0.8604 - val_acc: 0.7645\n",
      "Epoch 459/600\n",
      "1930/1930 [==============================] - 1s 679us/step - loss: 0.5472 - acc: 0.8523 - val_loss: 0.8390 - val_acc: 0.7757\n",
      "Epoch 460/600\n",
      "1408/1930 [====================>.........] - ETA: 0s - loss: 0.5484 - acc: 0.8500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-37e1f55b888c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m               validation_data=(x_val, y_val))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# plot train and validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=600,\n",
    "              validation_data=(x_val, y_val))\n",
    "# plot train and validation loss\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])\n",
    "pyplot.title('model train vs validation loss')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
